{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5719425-8fe6-4e6c-befd-fd45744b9362",
   "metadata": {},
   "source": [
    "### Name Entity Recognition\n",
    "##### Mar 24th 2023\n",
    "##### William Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f3df69c-63dc-4c3c-a59f-4d28de4b0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succesfully imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch import optim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "print(\"succesfully imported\")\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1cbd75e-96bd-4e54-9da1-d6f11ba59836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://yoseflaw.medium.com/step-by-step-ner-model-for-bahasa-indonesia-with-pytorch-and-torchtext-6f94fca08406\n",
    "# https://www.kaggle.com/code/ziliwang/baseline-pytorch-bilstm/input?select=train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fb934-bf91-4d67-bdf2-5c24eb4558d3",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51b4c1c-c5bd-446f-b55b-81e2f2935ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    1\n",
       "1     O\\n2 We O\\n3 do O\\n4 n't O\\n5 support O\\n6 an...\n",
       "2                                                    O\n",
       "Name: 74, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train',on_bad_lines='skip',sep=' ',header= None)\n",
    "data.iloc[74] #  a bad word; need to separate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65972596-1b09-49e9-b640-04befccc62d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>German</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124087</th>\n",
       "      <td>1</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124088</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124089</th>\n",
       "      <td>3</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124090</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124091</th>\n",
       "      <td>1</td>\n",
       "      <td>-DOCSTART-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124092 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1       2\n",
       "0       1          EU   B-ORG\n",
       "1       2     rejects       O\n",
       "2       3      German  B-MISC\n",
       "3       4        call       O\n",
       "4       5          to       O\n",
       "...    ..         ...     ...\n",
       "124087  1     Swansea   B-ORG\n",
       "124088  2           1       O\n",
       "124089  3     Lincoln   B-ORG\n",
       "124090  4           2       O\n",
       "124091  1  -DOCSTART-       O\n",
       "\n",
       "[124092 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336f2192-1033-421f-9f50-e55c663e278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change training and deving data to list of list of list\n",
    "def to_sentence(path):\n",
    "    df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2: # some line have corrupted content, for instance, line 74. So we need to clean it this way.\n",
    "                idx, word, NER = line.strip().split(\" \")\n",
    "                df.append([idx, word, NER])\n",
    "\n",
    "    df = pd.DataFrame(df, columns=['idx', 'word', 'NER'])\n",
    "    df = df.dropna()\n",
    "    X_train, y_train = [],[]\n",
    "    sent_X, sent_y = [],[]\n",
    "    temp = 1\n",
    "    for x in df.itertuples():\n",
    "        if(x.idx == '1' and temp == 0):\n",
    "            X_train.append(sent_X)\n",
    "            y_train.append(sent_y)\n",
    "            sent_X = []\n",
    "            sent_y = []\n",
    "        temp = 0\n",
    "        sent_X.append(x.word)\n",
    "        sent_y.append(x.NER)\n",
    "\n",
    "    X_train.append(sent_X)\n",
    "    y_train.append(sent_y)\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6910e153-87b8-46d7-baa5-52e9e3b166d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = to_sentence('train')\n",
    "X_dev, y_dev = to_sentence('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77722328-f329-4fdc-a153-b3b7b4a2e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence_test(path):\n",
    "    df = []\n",
    "    with open(path, 'r') as f:\n",
    "        for x in f.readlines():\n",
    "            if len(x) > 1: # some line have corrupted content, for instance, line 74. So we need to clean it this way.\n",
    "                idx, word= x.strip().split(\" \")\n",
    "                df.append([idx, word])\n",
    "\n",
    "    df = pd.DataFrame(df, columns=['idx', 'word'])\n",
    "    df = df.dropna()\n",
    "    X_test=[]\n",
    "    sent_X= []\n",
    "    temp = 1\n",
    "    for x in df.itertuples():\n",
    "        if(x.idx == '1' and temp == 0):\n",
    "            X_test.append(sent_X)\n",
    "            sent_X = []\n",
    "        temp = 0\n",
    "        sent_X.append(x.word)\n",
    "\n",
    "\n",
    "    X_test.append(sent_X)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba2361d-3644-4c04-8b30-c80067c00750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = to_sentence_test('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12908e7-183e-4877-8e44-27dd92ae7b23",
   "metadata": {},
   "source": [
    "#### Make sentence to numbers by creating macthing dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0abcd29-791e-4101-bdbd-58887a95ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict= dict()\n",
    "def create_dictionary(data1,data2,data3,vocabulary):\n",
    "    data = [data1,data2,data3]\n",
    "    idx = 2\n",
    "    vocab_dict[\"<pad>\"]=0\n",
    "    vocab_dict[\"<unk>\"]=1\n",
    "    \n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                if k not in vocab_dict:\n",
    "                    vocab_dict[k]= idx\n",
    "                    idx+=1\n",
    "                else:\n",
    "                    continue\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a028eb91-1a5a-4669-aeb7-1a11f65b25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary2(data,vocabulary):\n",
    "    idx = 2\n",
    "    vocab_dict[\"<pad>\"]=0\n",
    "    vocab_dict[\"<unk>\"]=1\n",
    "    \n",
    "    for i in data:\n",
    "            for k in i:\n",
    "                if k not in vocab_dict:\n",
    "                    vocab_dict[k]= idx\n",
    "                    idx+=1\n",
    "                else:\n",
    "                    continue\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea99a734-2b64-4c2c-9116-f375fc29d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict2 = dict()\n",
    "vocab_dict2 = create_dictionary2(X_train,vocab_dict2)\n",
    "#len(vocab_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb40eae-4bb7-4be2-b3e2-98bdba2b2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = create_dictionary(X_train,X_dev,X_test,vocab_dict)\n",
    "# vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3724601-0b35-4b8c-94cf-3c1c871bac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_num_data(data,dictionary):\n",
    "    integer_list = []\n",
    "    for sub in data:\n",
    "        integer_sub = []\n",
    "        for word in sub:\n",
    "            integer_sub.append(dictionary[word])\n",
    "        integer_list.append(integer_sub)\n",
    "    return integer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabc5ab9-fc6c-4ce9-b6cc-97fa2d61963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = transform_to_num_data(X_train,vocab_dict)\n",
    "X_dev_num = transform_to_num_data(X_dev,vocab_dict)\n",
    "X_test_num = transform_to_num_data(X_test,vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed9e801-08ab-4853-beda-e9b0c9d05d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to pass one set of data as NER dict should be short and the same\n",
    "def NER_dict(data):\n",
    "    idx = 0\n",
    "    ner_dict = dict()\n",
    "    ner = list(data[\"NER\"])\n",
    "    for i in ner:\n",
    "        if i not in ner_dict:\n",
    "            ner_dict[i]=idx\n",
    "            idx+=1\n",
    "        else:\n",
    "            continue\n",
    "    return ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57d0972-c3dd-44f9-9e5d-69b8161d1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df data\n",
    "df = list()\n",
    "with open('train', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line) > 2: # some line have corrupted content, for instance, line 74. So we need to clean it this way.\n",
    "            idx, word, NER = line.strip().split(\" \")\n",
    "            df.append([idx, word, NER])\n",
    "df = pd.DataFrame(df, columns=['idx', 'word', 'NER'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39dc3725-18a8-49c7-9bd0-fdbb2c58d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = NER_dict(df)\n",
    "#ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f02ba5-b375-43c4-8103-5e9d0cfb46e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cacf4b6-e5ce-44b4-aef0-a7a795304dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_num = transform_to_num_data(y_train,ner_dict)\n",
    "y_dev_num = transform_to_num_data(y_dev,ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb02afd-0b6b-49f4-bdc4-b78b3ca2e560",
   "metadata": {},
   "source": [
    "### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c75f19-f617-4bf6-bd44-20daa6174c4d",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba107d28-7959-44a7-aa5f-01d56d08e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, output_dim, hidden_dim, lstm_layers, bidirectional, dropout,tag_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tag_size = tag_size\n",
    "        self.lstm_layer = lstm_layers\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim,padding_idx=0)\n",
    "        # Bi-LSTM\n",
    "        self.blstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first =True\n",
    "        )\n",
    "        #Linear\n",
    "        self.fc = nn.Linear(hidden_dim *2 , output_dim) # bidrectional lstm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # ELU\n",
    "        self.elu = nn.ELU()\n",
    "        # classifier\n",
    "        self.classifier = nn.Linear(output_dim,tag_size)  # times 2 for bidirectional\n",
    "        \n",
    "    def forward(self,text):\n",
    "        embedding_out = self.dropout(self.embedding(text))\n",
    "        lstm_out, (hidden,cell) = self.blstm(embedding_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = self.elu(self.fc(lstm_out))\n",
    "        pred = self.classifier(lstm_out)\n",
    "        return pred\n",
    "\n",
    "    # count the number of parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(x.numel() for x in self.parameters() if x.requires_grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c170a436-4f25-4b5c-a350-99548825eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 100\n",
    "num_lstm_layer = 1\n",
    "hidden_dimension = 256\n",
    "dropout = 0.33\n",
    "output_dimension = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc823b66-b918-46d2-a1e7-9606b03752b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters is:  3829209\n",
      "BiLSTM(\n",
      "  (embedding): Embedding(30292, 100, padding_idx=0)\n",
      "  (blstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM(\n",
    "    input_dim = len(vocab_dict),#input dimension\n",
    "    embedding_dim = embedding_dimension, #embedding dimension\n",
    "    output_dim = output_dimension, # output_dimension\n",
    "    hidden_dim = hidden_dimension, #hidden dimension\n",
    "    lstm_layers = num_lstm_layer,#lstm_layers\n",
    "    bidirectional= True,#bidirectional\n",
    "    dropout = dropout,#dropout\n",
    "    tag_size = len(ner_dict)#tag_size\n",
    ")\n",
    "# input_dim, embedding_dim, hidden_dim, output_dim, lstm_layers, bidirectional, dropout,tag_size\n",
    "number_pf_parameters = bilstm.count_parameters()\n",
    "print(\"The number of trainable parameters is: \",number_pf_parameters)\n",
    "print(bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81625346-0ff0-4f46-97c1-170ba665457b",
   "metadata": {},
   "source": [
    "Convert training, dev, and test to loader mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d80181f0-3e29-4a20-ad11-d29fb915b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the texts so that they have the same length\n",
    "def padding(text,length,num):\n",
    "    padded_x = []\n",
    "    for row in text:\n",
    "        if len(row) > length:\n",
    "            padded_x.append(row[:length]) \n",
    "        else:\n",
    "            padded_row = row + [num]*(length-len(row))  \n",
    "            padded_x.append(padded_row)\n",
    "        \n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc61f6c0-4406-40ef-869c-b30f5f6b332e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a dataset and dataloader\n",
    "tempX = padding(X_train_num, 120,0)\n",
    "tempy = padding(y_train_num, 120,-1)\n",
    "\n",
    "X_train_tensor = torch.LongTensor(tempX)\n",
    "y_train_tensor = torch.LongTensor(tempy)\n",
    "\n",
    "train_tensor = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_tensor, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b189809-831b-48d9-9285-f2f344d8925e",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f609f88e-3866-4e15-8ac1-ad1daece1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(pred, y, ner_pad, words, pred_table):\n",
    "    counter = correct = 0\n",
    "    max_pred = pred.argmax(dim=1, keepdim=True) \n",
    "    temp_tuple = zip(max_pred, y, words)\n",
    "    for p, r, w in temp_tuple:\n",
    "        if r.item() == ner_pad:\n",
    "            continue\n",
    "        pred_table.append((w.item(), p.item(), r.item()))\n",
    "        if r.item() == p.item():\n",
    "            correct += 1\n",
    "        counter += 1\n",
    "    return counter, correct, pred_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7412ffe3-ba76-43a4-8886-1412c4f925d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, pred_table,optimizer):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    counter_total = 0\n",
    "    model.train()\n",
    "    for word, ner in iterator:   \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(word)\n",
    "        preds = preds.view(-1, preds.shape[-1])\n",
    "        ner = ner.view(-1) \n",
    "        loss = criterion(preds, ner)\n",
    "        counter, correct, pred_table = cal_accuracy(preds, ner, ner_pad, word.view(-1), pred_table)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += correct\n",
    "        counter_total += counter\n",
    "        \n",
    "    avg_los = epoch_loss / len(iterator)\n",
    "    avg_accuracy = epoch_acc / counter_total\n",
    "    return avg_los, avg_accuracy, pred_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fddf384-a618-4b4c-b25b-7163c3d2be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, pred_table,criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    counter_total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for word, ner in iterator:\n",
    "            preds = model(word)\n",
    "            # need reshape\n",
    "            preds = preds.view(-1, preds.shape[-1])\n",
    "            ner = ner.view(-1)\n",
    "            \n",
    "            loss = criterion(preds, ner)\n",
    "\n",
    "            counter, correct, pred_table = cal_accuracy(preds, ner, ner_pad, word.view(-1), pred_table)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += correct\n",
    "            counter_total += counter\n",
    "            \n",
    "    avg_los = epoch_loss / len(iterator)\n",
    "    avg_accuracy = epoch_acc / counter_total\n",
    "        \n",
    "    return avg_los, avg_accuracy, pred_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f2e06-d7d2-4d6b-b5e8-80256f3a4cf0",
   "metadata": {},
   "source": [
    "#### dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab54a781-67ea-4487-a691-a4abcc27b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdevX = padding(X_dev_num, 120,0)\n",
    "tempdevy = padding(y_dev_num, 120,-1)\n",
    "X_dev_tensor = torch.LongTensor(tempdevX)\n",
    "y_dev_tensor = torch.LongTensor(tempdevy)\n",
    "\n",
    "dev_tensor = TensorDataset(X_dev_tensor, y_dev_tensor)\n",
    "dev_loader = DataLoader(dev_tensor, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9233f6ce-45b8-42cf-95f8-04e2ba8550ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a index to Ner tag dictionary.\n",
    "idx_ner = dict()\n",
    "for k, v in ner_dict.items():\n",
    "    idx_ner[v]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5511a67c-5a2a-45e0-8c86-a38bde19b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 20\n",
    "ner_pad= -1\n",
    "optimizer = optim.SGD(bilstm.parameters(), lr=0.08, momentum=0.9,dampening=0.1) # SGD is the Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= -1)\n",
    "temp_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ddafaf5-f5e4-471f-bd17-6f2d73219347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t Trn Loss: 0.721 |  Trn Acc: 84.17%\n",
      "\t Val Loss: 0.525 |    Val Acc: 86.51%\n",
      "Epoch: 02\n",
      "\t Trn Loss: 0.516 |  Trn Acc: 86.78%\n",
      "\t Val Loss: 0.386 |    Val Acc: 89.38%\n",
      "Epoch: 03\n",
      "\t Trn Loss: 0.413 |  Trn Acc: 88.58%\n",
      "\t Val Loss: 0.315 |    Val Acc: 91.06%\n",
      "Epoch: 04\n",
      "\t Trn Loss: 0.348 |  Trn Acc: 89.87%\n",
      "\t Val Loss: 0.278 |    Val Acc: 91.91%\n",
      "Epoch: 05\n",
      "\t Trn Loss: 0.308 |  Trn Acc: 90.69%\n",
      "\t Val Loss: 0.261 |    Val Acc: 92.41%\n",
      "Epoch: 06\n",
      "\t Trn Loss: 0.277 |  Trn Acc: 91.46%\n",
      "\t Val Loss: 0.250 |    Val Acc: 92.78%\n",
      "Epoch: 07\n",
      "\t Trn Loss: 0.256 |  Trn Acc: 91.91%\n",
      "\t Val Loss: 0.238 |    Val Acc: 93.09%\n",
      "Epoch: 08\n",
      "\t Trn Loss: 0.240 |  Trn Acc: 92.32%\n",
      "\t Val Loss: 0.229 |    Val Acc: 93.41%\n",
      "Epoch: 09\n",
      "\t Trn Loss: 0.223 |  Trn Acc: 92.72%\n",
      "\t Val Loss: 0.225 |    Val Acc: 93.52%\n",
      "Epoch: 10\n",
      "\t Trn Loss: 0.211 |  Trn Acc: 93.00%\n",
      "\t Val Loss: 0.224 |    Val Acc: 93.61%\n",
      "Epoch: 11\n",
      "\t Trn Loss: 0.200 |  Trn Acc: 93.31%\n",
      "\t Val Loss: 0.222 |    Val Acc: 93.78%\n",
      "Epoch: 12\n",
      "\t Trn Loss: 0.192 |  Trn Acc: 93.57%\n",
      "\t Val Loss: 0.227 |    Val Acc: 93.67%\n",
      "Epoch: 13\n",
      "\t Trn Loss: 0.182 |  Trn Acc: 93.79%\n",
      "\t Val Loss: 0.220 |    Val Acc: 93.87%\n",
      "Epoch: 14\n",
      "\t Trn Loss: 0.178 |  Trn Acc: 93.88%\n",
      "\t Val Loss: 0.213 |    Val Acc: 94.01%\n",
      "Epoch: 15\n",
      "\t Trn Loss: 0.172 |  Trn Acc: 94.14%\n",
      "\t Val Loss: 0.217 |    Val Acc: 94.09%\n",
      "Epoch: 16\n",
      "\t Trn Loss: 0.167 |  Trn Acc: 94.24%\n",
      "\t Val Loss: 0.218 |    Val Acc: 94.18%\n",
      "Epoch: 17\n",
      "\t Trn Loss: 0.159 |  Trn Acc: 94.45%\n",
      "\t Val Loss: 0.220 |    Val Acc: 94.13%\n",
      "Epoch: 18\n",
      "\t Trn Loss: 0.153 |  Trn Acc: 94.61%\n",
      "\t Val Loss: 0.218 |    Val Acc: 94.16%\n",
      "Epoch: 19\n",
      "\t Trn Loss: 0.150 |  Trn Acc: 94.69%\n",
      "\t Val Loss: 0.215 |    Val Acc: 94.20%\n",
      "Epoch: 20\n",
      "\t Trn Loss: 0.145 |  Trn Acc: 94.84%\n",
      "\t Val Loss: 0.217 |    Val Acc: 94.26%\n"
     ]
    }
   ],
   "source": [
    "#predict_result_dev = run_training(epoch_num,bilstm,train_loader,dev_loader)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    temp_train = list()\n",
    "    temp_test = list()\n",
    "   \n",
    "    train_loss, train_acc, train_pred_table = train(bilstm, train_loader, temp_train,optimizer)\n",
    "    val_loss, val_acc, val_pred_table = evaluate(bilstm, dev_loader, temp_test,criterion)\n",
    "\n",
    "    if val_loss <= float('inf'):\n",
    "        temp_loss = val_loss\n",
    "        predict_result = val_pred_table\n",
    "        torch.save(bilstm.state_dict(), 'blstm1.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\t Trn Loss: {train_loss:.3f} |  Trn Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val Loss: {val_loss:.3f} |    Val Acc: {val_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "365a187f-107e-4ee8-a64d-3b5325af3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"dev\",\"r\") as dev, open(\"dev1.out\",\"w\") as dev1_out:\n",
    "        y_dev_pred = []\n",
    "        for i in predict_result:\n",
    "            y_dev_pred.append(int(i[1]))\n",
    "        temp =0\n",
    "        for x in dev:\n",
    "            x = x.strip()\n",
    "            if x:\n",
    "                idx,ner = x.split(\" \")[:2]\n",
    "                pred_ner = idx_ner[y_dev_pred[temp]]\n",
    "                temp+=1\n",
    "                dev1_out.write(f\"{idx} {ner} {pred_ner}\\n\")\n",
    "            else:\n",
    "                dev1_out.write(\"\\n\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67f1ab06-fde9-4c58-b4ff-78348dd6de0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5316 phrases; correct: 4053.\n",
      "accuracy:  94.26%; precision:  76.24%; recall:  68.21%; FB1:  72.00\n",
      "              LOC: precision:  82.92%; recall:  77.68%; FB1:  80.21  1721\n",
      "             MISC: precision:  76.83%; recall:  71.91%; FB1:  74.29  863\n",
      "              ORG: precision:  64.32%; recall:  60.63%; FB1:  62.42  1264\n",
      "              PER: precision:  78.34%; recall:  62.43%; FB1:  69.49  1468\n"
     ]
    }
   ],
   "source": [
    "## for perl testing:\n",
    "try:\n",
    "    with open(\"dev\",\"r\") as dev, open(\"dev1_perl.out\",\"w\") as dev1_out:\n",
    "        y_dev_pred = []\n",
    "        for i in predict_result:\n",
    "            y_dev_pred.append(int(i[1]))\n",
    "        temp2 =0\n",
    "        for x in dev:\n",
    "            x = x.strip()\n",
    "            if x:\n",
    "                item = x.split(\" \")\n",
    "                idx,word,ner = item[0],item[1],item[2]\n",
    "                pred_ner = idx_ner[y_dev_pred[temp2]]\n",
    "                temp2+=1\n",
    "                dev1_out.write(f\"{idx} {word} {ner} {pred_ner}\\n\")\n",
    "            else:\n",
    "                dev1_out.write(\"\\n\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")\n",
    "!perl conll03eval.txt < dev1_perl.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb44835-2b85-4074-a4c4-af7fc617876a",
   "metadata": {},
   "source": [
    "#### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5142e2e0-948f-466d-b9bb-e1728b64487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temptestX = padding(X_test_num, 120,0)\n",
    "X_test_tensor = torch.LongTensor(temptestX)\n",
    "test_loader = DataLoader(X_test_tensor, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d99d251e-9798-47e1-bfc7-8447451fc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_evaluate2(preds, words, pred_result):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    temp_tuple = zip(max_preds, words)\n",
    "    for p, w in temp_tuple:\n",
    "        if w == 0:\n",
    "            continue\n",
    "        else:\n",
    "            pred_result.append((w, p[0]))\n",
    "\n",
    "    return pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9344df9-3c51-4037-9b9b-5f895b9eb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(model, iterator, pred_table):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for word in iterator:\n",
    "            \n",
    "            pred = model(word)\n",
    "            pred = pred.view(-1, pred.shape[-1])\n",
    "\n",
    "            pred_table = cal_evaluate2(pred, word.view(-1), pred_table)\n",
    "\n",
    "    return pred_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c873de0-eafb-40f8-87b6-d0554b2318ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result2 = []\n",
    "pred_result2 = evaluate2(bilstm, test_loader, pred_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ad886a5-de32-4269-8b79-e4b7b6f623e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"test\",\"r\") as test, open(\"test1.out\",\"w\") as test1_out:\n",
    "        y_test_pred = []\n",
    "        temp4 =0\n",
    "        for i in pred_result2:\n",
    "            y_test_pred.append(int(i[1]))\n",
    "        for x in test:\n",
    "            x = x.strip()\n",
    "            if x and temp4<len(y_test_pred):\n",
    "                idx, word = x.split()[:2]\n",
    "                #idx, word = x[0],x[1]\n",
    "                pred_ner = idx_ner[y_test_pred[temp4]]\n",
    "                temp4+=1\n",
    "                test1_out.write(f\"{idx} {word} {pred_ner}\\n\")\n",
    "            else:\n",
    "                test1_out.write(\"\\n\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc4ccc-a216-4129-9371-3488125203a8",
   "metadata": {},
   "source": [
    "### BiLSTM with GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18373293-aa74-4664-92bf-de314d08ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_csv('glove.6B.100d', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "#glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d782ca05-88d1-4f35-b321-ea74ebc3a4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the glove dataframe to be like a dictionary where each word is the key.\n",
    "glove2 =glove.T\n",
    "glove_dict = dict()\n",
    "for k,v in glove2.items():\n",
    "    glove_dict[k] = v.values\n",
    "# glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "899b2b28-6a58-42cd-b2fc-133f09634feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embedding matrix should be like (length of vocab dict, embedding dimension)\n",
    "def embedding_matrix(embedding_size,vocab_dict,glove_vec):\n",
    "    width  = int(len(vocab_dict))\n",
    "    embedding_matrix = np.zeros((width,embedding_size))\n",
    "    for w, j in vocab_dict.items():\n",
    "        embedding_vec = glove_vec.get(w.lower())\n",
    "        if embedding_vec is not None:\n",
    "            embedding_matrix[j] = embedding_vec\n",
    "        \n",
    "    embedding_matrix = torch.LongTensor(embedding_matrix)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96098a1e-6384-410f-acd5-c429bc053c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_matrix(100,vocab_dict,glove_dict)\n",
    "#embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bb8c8ac-e045-48ce-b369-bd7099459e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db22d77f-edc4-4b3e-98ca-25d031c5c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_glove(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, output_dim, hidden_dim, lstm_layers, bidirectional, dropout,tag_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tag_size = tag_size\n",
    "        self.lstm_layer = lstm_layers\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim,padding_idx=0)\n",
    "        # Bi-LSTM\n",
    "        self.blstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first =True\n",
    "        )\n",
    "        #Linear\n",
    "        self.fc = nn.Linear(hidden_dim *2 , output_dim) # bidrectional lstm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # ELU\n",
    "        self.elu = nn.ELU()\n",
    "        # classifier\n",
    "        self.classifier = nn.Linear(output_dim,tag_size)  # times 2 for bidirectional\n",
    "        \n",
    "    def forward(self,text):\n",
    "        embedding_out = self.dropout(self.embedding(text))\n",
    "        lstm_out, (hidden,cell) = self.blstm(embedding_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = self.elu(self.fc(lstm_out))\n",
    "        pred = self.classifier(lstm_out)\n",
    "        return pred\n",
    "    \n",
    "     # initialize all parameters from normal distribution for better converging during training\n",
    "\n",
    "    # count the number of parameters\n",
    "    def count_parameters(self):\n",
    "        return sum(x.numel() for x in self.parameters() if x.requires_grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b31e2461-23a3-4913-9b95-85db9a6f29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters is:  3829209\n",
      "BiLSTM_glove(\n",
      "  (embedding): Embedding(30292, 100, padding_idx=0)\n",
      "  (blstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bilstm_glove = BiLSTM_glove(\n",
    "    input_dim = len(vocab_dict),#input dimension\n",
    "    embedding_dim = embedding_dimension, #embedding dimension\n",
    "    output_dim = output_dimension, # output_dimension\n",
    "    hidden_dim = hidden_dimension, #hidden dimension\n",
    "    lstm_layers = num_lstm_layer,#lstm_layers\n",
    "    bidirectional= True,#bidirectional\n",
    "    dropout = dropout,#dropout\n",
    "    tag_size = len(ner_dict)#tag_size\n",
    ")\n",
    "bilstm_glove.embedding.weight.data.copy_(embedding_matrix) # add embedding matrix\n",
    "# input_dim, embedding_dim, hidden_dim, output_dim, lstm_layers, bidirectional, dropout,tag_size):\n",
    "number_pf_parameters2 = bilstm_glove.count_parameters()\n",
    "#bilstm.to(device)\n",
    "print(\"The number of trainable parameters is: \",number_pf_parameters2)\n",
    "print(bilstm_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cc911c7-162e-4bdb-b0be-79c42749fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 20\n",
    "ner_pad=-1\n",
    "optimizer2 = optim.SGD(bilstm_glove.parameters(), lr=0.05, momentum=0.9, nesterov=True)#weight_decay=0.3\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2, 'min', patience=4)\n",
    "criterion2 = nn.CrossEntropyLoss(ignore_index= -1)\n",
    "temp_loss2 = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d27ab4a-3f9c-4492-95d6-4449e9219836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(epoch_num,model,training,testing,optim,criter,name):\n",
    "    for epoch in range(epoch_num):\n",
    "        temp_train = list()\n",
    "        temp_test = list()\n",
    "\n",
    "        train_loss, train_acc, train_pred_result = train(model, training, temp_train,optim)\n",
    "        val_loss, val_acc, val_pred_result = evaluate(model, testing, temp_test,criter)\n",
    "\n",
    "        if val_loss <= float('inf'):\n",
    "            temp_loss2 = val_loss\n",
    "            predict_result = val_pred_result\n",
    "            torch.save(bilstm.state_dict(), str(name))\n",
    "        scheduler.step(val_loss)\n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        print(f'\\t Trn Loss: {train_loss:.3f} |  Trn Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val Loss: {val_loss:.3f} |  Val Acc: {val_acc*100:.2f}%')\n",
    "    return predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efa58ca1-ef39-4c11-b598-ca7d0f4c69ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\t Trn Loss: 0.734 |  Trn Acc: 83.94%\n",
      "\t Val Loss: 0.548 |  Val Acc: 86.79%\n",
      "Epoch: 02\n",
      "\t Trn Loss: 0.417 |  Trn Acc: 88.36%\n",
      "\t Val Loss: 0.322 |  Val Acc: 91.15%\n",
      "Epoch: 03\n",
      "\t Trn Loss: 0.284 |  Trn Acc: 91.44%\n",
      "\t Val Loss: 0.257 |  Val Acc: 92.91%\n",
      "Epoch: 04\n",
      "\t Trn Loss: 0.217 |  Trn Acc: 93.22%\n",
      "\t Val Loss: 0.229 |  Val Acc: 93.69%\n",
      "Epoch: 05\n",
      "\t Trn Loss: 0.168 |  Trn Acc: 94.63%\n",
      "\t Val Loss: 0.214 |  Val Acc: 94.22%\n",
      "Epoch: 06\n",
      "\t Trn Loss: 0.137 |  Trn Acc: 95.49%\n",
      "\t Val Loss: 0.211 |  Val Acc: 94.52%\n",
      "Epoch: 07\n",
      "\t Trn Loss: 0.114 |  Trn Acc: 96.22%\n",
      "\t Val Loss: 0.211 |  Val Acc: 94.75%\n",
      "Epoch: 08\n",
      "\t Trn Loss: 0.097 |  Trn Acc: 96.72%\n",
      "\t Val Loss: 0.221 |  Val Acc: 94.76%\n",
      "Epoch: 09\n",
      "\t Trn Loss: 0.084 |  Trn Acc: 97.16%\n",
      "\t Val Loss: 0.222 |  Val Acc: 94.87%\n",
      "Epoch: 10\n",
      "\t Trn Loss: 0.072 |  Trn Acc: 97.49%\n",
      "\t Val Loss: 0.235 |  Val Acc: 94.93%\n",
      "Epoch: 11\n",
      "\t Trn Loss: 0.065 |  Trn Acc: 97.75%\n",
      "\t Val Loss: 0.232 |  Val Acc: 95.04%\n",
      "Epoch: 12\n",
      "\t Trn Loss: 0.056 |  Trn Acc: 98.01%\n",
      "\t Val Loss: 0.242 |  Val Acc: 95.17%\n",
      "Epoch: 13\n",
      "\t Trn Loss: 0.053 |  Trn Acc: 98.08%\n",
      "\t Val Loss: 0.246 |  Val Acc: 95.16%\n",
      "Epoch: 14\n",
      "\t Trn Loss: 0.053 |  Trn Acc: 98.08%\n",
      "\t Val Loss: 0.247 |  Val Acc: 95.16%\n",
      "Epoch: 15\n",
      "\t Trn Loss: 0.052 |  Trn Acc: 98.13%\n",
      "\t Val Loss: 0.247 |  Val Acc: 95.18%\n",
      "Epoch: 16\n",
      "\t Trn Loss: 0.051 |  Trn Acc: 98.13%\n",
      "\t Val Loss: 0.248 |  Val Acc: 95.19%\n",
      "Epoch: 17\n",
      "\t Trn Loss: 0.049 |  Trn Acc: 98.19%\n",
      "\t Val Loss: 0.244 |  Val Acc: 95.24%\n",
      "Epoch: 18\n",
      "\t Trn Loss: 0.050 |  Trn Acc: 98.18%\n",
      "\t Val Loss: 0.242 |  Val Acc: 95.26%\n",
      "Epoch: 19\n",
      "\t Trn Loss: 0.049 |  Trn Acc: 98.23%\n",
      "\t Val Loss: 0.242 |  Val Acc: 95.26%\n",
      "Epoch: 20\n",
      "\t Trn Loss: 0.050 |  Trn Acc: 98.20%\n",
      "\t Val Loss: 0.242 |  Val Acc: 95.27%\n"
     ]
    }
   ],
   "source": [
    "result_golve = run_training(20,bilstm_glove,train_loader,dev_loader,optimizer2,criterion2,'blstm2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92a224e4-30ed-413b-b5f0-ef8a5a06b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"dev\",\"r\") as dev, open(\"dev2.out\",\"w\") as dev2_out:\n",
    "        y_dev_pred_g = []\n",
    "        for i in result_golve:\n",
    "            y_dev_pred_g.append(int(i[1]))\n",
    "        temp6 =0\n",
    "        for x in dev:\n",
    "            x = x.strip()\n",
    "            if x:\n",
    "                idx,ner = x.split(\" \")[:2]\n",
    "                pred_ner = idx_ner[y_dev_pred_g[temp6]]\n",
    "                temp6+=1\n",
    "                dev2_out.write(f\"{idx} {ner} {pred_ner}\\n\")\n",
    "            else:\n",
    "                dev2_out.write(\"\\n\")\n",
    "                \n",
    "        print(\"success\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05f69659-2545-4299-b6b8-42d9f05b3dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "processed 51578 tokens with 5942 phrases; found: 5860 phrases; correct: 4510.\n",
      "accuracy:  95.27%; precision:  76.96%; recall:  75.90%; FB1:  76.43\n",
      "              LOC: precision:  82.93%; recall:  84.87%; FB1:  83.88  1880\n",
      "             MISC: precision:  75.54%; recall:  75.38%; FB1:  75.46  920\n",
      "              ORG: precision:  69.25%; recall:  70.02%; FB1:  69.63  1356\n",
      "              PER: precision:  77.29%; recall:  71.50%; FB1:  74.28  1704\n"
     ]
    }
   ],
   "source": [
    "## for perl testing:\n",
    "try:\n",
    "    with open(\"dev\",\"r\") as dev, open(\"dev2_perl.out\",\"w\") as dev2_out:\n",
    "        y_dev_pred_g2 = []\n",
    "        for i in result_golve:\n",
    "            y_dev_pred_g2.append(int(i[1]))\n",
    "        temp5 =0\n",
    "        for x in dev:\n",
    "            x = x.strip()\n",
    "            if x:\n",
    "                item = x.split(\" \")\n",
    "                idx,word,ner = item[0],item[1],item[2]\n",
    "                pred_ner = idx_ner[y_dev_pred_g2[temp5]]\n",
    "                temp5+=1\n",
    "                dev2_out.write(f\"{idx} {word} {ner} {pred_ner}\\n\")\n",
    "            else:\n",
    "                dev2_out.write(\"\\n\")\n",
    "        print(\"success\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")\n",
    "!perl conll03eval.txt < dev2_perl.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d649c03c-fbd8-4c2f-98a1-c9480c15d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result2_g = []\n",
    "pred_result2_g = evaluate2(bilstm_glove, test_loader, pred_result2_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90961a44-6314-4448-afbd-f97aafaaaa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"test\",\"r\") as test, open(\"test2.out\",\"w\") as test2_out:\n",
    "        y_test_pred_g = []\n",
    "        temp8 =0\n",
    "        for i in pred_result2_g:\n",
    "            y_test_pred_g.append(int(i[1]))\n",
    "        for x in test:\n",
    "            x = x.strip()\n",
    "            if x and temp8<len(y_test_pred_g):\n",
    "                idx, word = x.split()[:2]\n",
    "                pred_ner = idx_ner[y_test_pred_g[temp8]]\n",
    "                temp8+=1\n",
    "                test2_out.write(f\"{idx} {word} {pred_ner}\\n\")\n",
    "            else:\n",
    "                test2_out.write(\"\\n\")\n",
    "        print(\"success\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4a4f90a-01e1-46b2-a1f1-9df3102cb189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nepoch_num = 20\\nner_pad=-1\\noptimizer3 = optim.SGD(bilstm_glove.parameters(), lr=0.01, momentum=0.9, nesterov=True)\\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer3, 'min', patience=4)\\ncriterion2 = nn.CrossEntropyLoss(ignore_index= -1)\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "epoch_num = 20\n",
    "ner_pad=-1\n",
    "optimizer3 = optim.SGD(bilstm_glove.parameters(), lr=0.01, momentum=0.9, nesterov=True,weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer3, 'min', patience=4)\n",
    "criterion2 = nn.CrossEntropyLoss(ignore_index= -1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7222a2b-d4ca-46c7-b2fa-b1efd9766832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na= run_training(20,bilstm_glove,train_loader,dev_loader,optimizer3,criterion2,'blstm3.pt')\\na\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a= run_training(20,bilstm_glove,train_loader,dev_loader,optimizer3,criterion2,'blstm3.pt')\n",
    "a\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b69c0c7b-fea3-461d-af7e-052dc4f15972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## for perl testing:\\ntry:\\n    with open(\"dev\",\"r\") as dev, open(\"dev3_perl.out\",\"w\") as dev3_out:\\n        y_dev_pred_g3 = []\\n        for i in a:\\n            y_dev_pred_g3.append(int(i[1]))\\n        temp10 =0\\n        for x in dev:\\n            x = x.strip()\\n            if x:\\n                item = x.split(\" \")\\n                idx,word,ner = item[0],item[1],item[2]\\n                pred_ner = idx_ner[y_dev_pred_g3[temp10]]\\n                temp10+=1\\n                dev3_out.write(f\"{idx} {word} {ner} {pred_ner}\\n\")\\n            else:\\n                dev3_out.write(\"\\n\")\\nexcept IOError as error:\\n    print(\"There\\'s an error opening the file. Please correct the path. Thanks.\")\\n!perl conll03eval.txt < dev3_perl.out\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## for perl testing:\n",
    "try:\n",
    "    with open(\"dev\",\"r\") as dev, open(\"dev3_perl.out\",\"w\") as dev3_out:\n",
    "        y_dev_pred_g3 = []\n",
    "        for i in a:\n",
    "            y_dev_pred_g3.append(int(i[1]))\n",
    "        temp10 =0\n",
    "        for x in dev:\n",
    "            x = x.strip()\n",
    "            if x:\n",
    "                item = x.split(\" \")\n",
    "                idx,word,ner = item[0],item[1],item[2]\n",
    "                pred_ner = idx_ner[y_dev_pred_g3[temp10]]\n",
    "                temp10+=1\n",
    "                dev3_out.write(f\"{idx} {word} {ner} {pred_ner}\\n\")\n",
    "            else:\n",
    "                dev3_out.write(\"\\n\")\n",
    "except IOError as error:\n",
    "    print(\"There's an error opening the file. Please correct the path. Thanks.\")\n",
    "!perl conll03eval.txt < dev3_perl.out\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet capture",
   "language": "python",
   "name": "tweet_capture_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
